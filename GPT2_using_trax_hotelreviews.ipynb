{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2 using trax_hotelreviews.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOtAEzROlLXI"
      },
      "source": [
        "##using this as a guide: https://github.com/ngoquanghuy99/transformer-summarization/blob/main/Abstractive_summarization.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShIhWY1klTCp",
        "outputId": "b9cced09-2e37-448e-b29c-20b3f10a4eb8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aDWNK3XzxmN"
      },
      "source": [
        "#drive.mount(\"/content/drive\", force_remount=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv8oV8n4lTFN",
        "outputId": "2f5884e1-97db-4a33-9922-5d7c17f8c8a1"
      },
      "source": [
        "!pip -q install trax\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 637 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 31.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZxe4MmKlTHo"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as jnp\n",
        "\n",
        "# to print the entire np array\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyViHfjVlTKC"
      },
      "source": [
        "#import dataset \n",
        "\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU4FGQS3lTMD"
      },
      "source": [
        "#import pandas as pd\n",
        "\n",
        "#import io\n",
        "#news = pd.read_csv(io.BytesIO(uploaded['Datafiniti_Hotel_Reviews.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P72W09GBverR"
      },
      "source": [
        "news = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Summarization/data/Datafiniti_Hotel_Reviews.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZu4GLBSmSIM",
        "outputId": "dfc99745-c6be-48af-829e-0f9214463734"
      },
      "source": [
        "news.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zHeUIIQ7mSLA",
        "outputId": "2db60282-eb9a-48dd-e1f9-5ca35d267b16"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews.text</th>\n",
              "      <th>reviews.title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our experience at Rancho Valencia was absolute...</td>\n",
              "      <td>Best romantic vacation ever!!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Amazing place. Everyone was extremely warm and...</td>\n",
              "      <td>Sweet sweet serenity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We booked a 3 night stay at Rancho Valencia to...</td>\n",
              "      <td>Amazing Property and Experience</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Currently in bed writing this for the past hr ...</td>\n",
              "      <td>Never again...beware, if you want sleep.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I live in Md and the Aloft is my Home away fro...</td>\n",
              "      <td>ALWAYS GREAT STAY...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        reviews.text                             reviews.title\n",
              "0  Our experience at Rancho Valencia was absolute...           Best romantic vacation ever!!!!\n",
              "1  Amazing place. Everyone was extremely warm and...                      Sweet sweet serenity\n",
              "2  We booked a 3 night stay at Rancho Valencia to...           Amazing Property and Experience\n",
              "3  Currently in bed writing this for the past hr ...  Never again...beware, if you want sleep.\n",
              "4  I live in Md and the Aloft is my Home away fro...                      ALWAYS GREAT STAY..."
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPxYeTDQmoxx"
      },
      "source": [
        "news = news.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc7N6x56mo4w"
      },
      "source": [
        "news['reviews.title'] = news['reviews.title'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFu5vPLOmo9I"
      },
      "source": [
        "news['reviews.text'] = news['reviews.text'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_t-N1VrmSNd"
      },
      "source": [
        "document = news['reviews.text']\n",
        "summary = news['reviews.title']\n",
        "def create_data_stream(document, summary):\n",
        "  for doc, sum in zip(document, summary):\n",
        "    yield doc, sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYtpzXHQmSQM",
        "outputId": "626517ff-e7ab-426d-84fd-864bb3e388e7"
      },
      "source": [
        "portion = 0.96\n",
        "index_portion = int(len(document) * 0.96)\n",
        "print(index_portion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPKb8P_zmSSb"
      },
      "source": [
        "train_document = news['reviews.text'][:9998]\n",
        "train_summary = news['reviews.title'][:9998]\n",
        "eval_document = news['reviews.text'][9998:]\n",
        "eval_summary = news['reviews.title'][9998:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpS02OVzmSUi"
      },
      "source": [
        "train_document = [bytes(doc, 'utf-8') for doc in train_document]\n",
        "train_summary = [bytes(summ, 'utf-8') for summ in train_summary]\n",
        "eval_document = [bytes(doc, 'utf-8') for doc in eval_document]\n",
        "eval_summary = [bytes(summ, 'utf-8') for summ in eval_summary]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TSBdg8imSW2"
      },
      "source": [
        "train_data_stream = create_data_stream(train_document, train_summary)\n",
        "eval_data_stream = create_data_stream(eval_document, eval_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ZcsCY9mSZR",
        "outputId": "a1000bf0-d461-428f-ef26-7dd1c8eb0389"
      },
      "source": [
        "print(next(train_data_stream))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(b'our experience at rancho valencia was absolutely perfect from beginning to end!!!! we felt special and very happy during our stayed. i would come back in a heart beat!!!', b'best romantic vacation ever!!!!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgxG_Db-mSb0",
        "outputId": "5eedea8f-51b4-4369-eab3-f903320bd2a5"
      },
      "source": [
        "print(type(next(train_data_stream)[1]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'bytes'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTYzRjqxOB7"
      },
      "source": [
        "#import os\n",
        "#import numpy as np\n",
        "\n",
        "#!pip install -q -U trax\n",
        "#import trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIbJ8-9IlTOZ"
      },
      "source": [
        "##tokenize & detokenize helper functions\n",
        "def tokenize(input_str, EOS=1):\n",
        "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "  \n",
        "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "    # we get around it by making a 1-element stream with `iter`.\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_dir='/content/drive/MyDrive/Colab Notebooks/Summarization/vocab_dir',\n",
        "                                      vocab_file='summarize32k.subword.subwords.txt'))\n",
        "    \n",
        "    # Mark the end of the sentence with EOS\n",
        "    return list(inputs) + [EOS]\n",
        "\n",
        "def detokenize(integers):\n",
        "    \"\"\"List of ints to str\"\"\"\n",
        "  \n",
        "    s = trax.data.detokenize(integers,\n",
        "                             vocab_dir='/content/drive/MyDrive/Colab Notebooks/Summarization/vocab_dir',\n",
        "                             vocab_file='summarize32k.subword.subwords.txt')\n",
        "    \n",
        "    return wrapper.fill(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU-FxtB0o2HA",
        "outputId": "871fda90-b621-4c95-bbcd-3005353c0e29"
      },
      "source": [
        "# test \n",
        "print(tokenize('why you did that shit to me, man?', EOS=1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11406, 33, 206, 285, 12803, 320, 156, 2, 157, 3882, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sCcsn0Ko2Jz"
      },
      "source": [
        "# Special tokens\n",
        "SEP = 0 # Padding or separator token\n",
        "EOS = 1 # End of sentence token\n",
        "\n",
        "# Concatenate tokenized inputs and targets using 0 as separator.\n",
        "def preprocess(stream):\n",
        "    for (article, summary) in stream:\n",
        "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
        "        yield joint, joint, np.array(mask)\n",
        "\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\n",
        "input_pipeline = trax.data.Serial(\n",
        "    # Tokenizes\n",
        "    trax.data.Tokenize(vocab_dir='/content/drive/MyDrive/Colab Notebooks/Summarization/vocab_dir',\n",
        "                       vocab_file='summarize32k.subword.subwords.txt'),\n",
        "    # Uses function defined above\n",
        "    preprocess,\n",
        "    # Filters out examples longer than 2048\n",
        "    trax.data.FilterByLength(2048)\n",
        ")\n",
        "\n",
        "# Apply preprocessing to data streams.\n",
        "train_stream = input_pipeline(train_data_stream)\n",
        "eval_stream = input_pipeline(eval_data_stream)\n",
        "\n",
        "train_input, train_target, train_mask = next(train_stream)\n",
        "\n",
        "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cHbi42wo2Mj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4a95b1-b72e-4e8f-c8d7-8e8e53ec04b5"
      },
      "source": [
        "# prints mask, 0s on article, 1s on summary\n",
        "print(f'Single example mask:\\n\\n {train_mask}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single example mask:\n",
            "\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DtwHOBfo2Ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7e8d41-56af-472b-947d-7b305fbad46f"
      },
      "source": [
        "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
        "print(f'Single example:\\n\\n {detokenize(train_input)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single example:\n",
            "\n",
            " we booked a 3 night stay at rancho valencia to play some tennis, since\n",
            "it is one of the highest rated tennis resorts in america. this place\n",
            "is really over the top from a luxury standpoint and overall\n",
            "experience. the villas are really perfect, the staff is great,\n",
            "attention to details (includes fresh squeezed orange juice each\n",
            "morning), restaurants, bar and room service amazing, and the tennis\n",
            "program was really impressive as well. we will want to come back here\n",
            "again.<EOS><pad>amazingproperty and experience<EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Kr-UEgo2Qk"
      },
      "source": [
        "##batching with bucketing \n",
        "\n",
        "# Bucketing to create batched generators.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256,\n",
        "# 4 of length < 512. And so on. \n",
        "boundaries =  [128, 256,  512, 1024]\n",
        "batch_sizes = [16,    8,    4,    2, 1]\n",
        "\n",
        "# Create the streams.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes)(train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBhlKmF_o2Sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0226137-6c88-4eb4-d827-5eab26082b43"
      },
      "source": [
        "# Every execution will result in generation of a different article\n",
        "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
        "input_batch, _, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# Shape of the input_batch\n",
        "input_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW0p5jcvo2VO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6189a49-b66d-4dba-878d-e3fb50bb026d"
      },
      "source": [
        "# print corresponding integer values\n",
        "print(input_batch[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  950   132  2460   970   824  1019   213   543 11066    32    37    42\n",
            "    77    18    46  5324 18028    16   186 17459 17125   140   922   213\n",
            "   979 14129   320 21674  8122   320  1151   793    77     7     5  1290\n",
            "    41    49   124     3 23962     4    10   865   186   100    49     7\n",
            "    26  7678    10     1     0   369   457   841  3725  6278     2   122\n",
            "    33   483  7678    10     1     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlVOtv18o2ZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db301a33-cc13-4802-e0f0-fdeaabcb8388"
      },
      "source": [
        "# test linh tinh \n",
        "print(mask_batch[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-EoFyHdj1Vp",
        "outputId": "134e288b-55b4-450f-ce4e-08ea38f6f3d3"
      },
      "source": [
        "# print the article and its summary\n",
        "print('Article:\\n\\n', detokenize(input_batch[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article:\n",
            "\n",
            " currently in bed writing this for the past hr 1/2 there have been dogs\n",
            "barking and squealing call the front desk to advise basically to be\n",
            "told there's nothing they can do. 315.00 and i can't\n",
            "sleep.<EOS><pad>neveragain...beware, if you want sleep.<EOS><pad><pad>\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9twliFBj1Yx"
      },
      "source": [
        "def DotProductAttention(query, key, value, mask):\n",
        "    \"\"\"Dot product self-attention.\n",
        "    Args:\n",
        "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
        "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
        "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
        "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
        "\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
        "    \"\"\"\n",
        "\n",
        "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
        "\n",
        "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
        "    depth = query.shape[-1]\n",
        "\n",
        "    # Calculate scaled query key dot product according to formula above\n",
        "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
        "    \n",
        "    # Apply the mask\n",
        "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
        "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
        "    \n",
        "    # Softmax formula implementation\n",
        "    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
        "    # Hint: Last axis should be used and keepdims should be True\n",
        "    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
        "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
        "\n",
        "    # Take exponential of dots minus logsumexp to get softmax\n",
        "    # Use jnp.exp()\n",
        "    dots = jnp.exp(dots-logsumexp)\n",
        "\n",
        "    # Multiply dots by value to get self-attention\n",
        "    # Use jnp.matmul()\n",
        "    attention = jnp.matmul(dots, value)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T1JsF0Tj1bj"
      },
      "source": [
        "def compute_attention_heads_closure(n_heads, d_head):\n",
        "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
        "    Args:\n",
        "        d_head (int):  dimensionality of heads.\n",
        "        n_heads (int): number of attention heads.\n",
        "    Returns:\n",
        "        function: compute_attention_heads function\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_attention_heads(x):\n",
        "        \"\"\" Compute the attention heads.\n",
        "        Args:\n",
        "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "        Returns:\n",
        "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        # Length of the sequence\n",
        "        # Should be size of x's first dimension without counting the batch dim\n",
        "        seqlen = x.shape[1]\n",
        "        # Reshape x using jnp.reshape()\n",
        "        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
        "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
        "        # Transpose x using jnp.transpose()\n",
        "        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
        "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
        "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "        # Reshape x using jnp.reshape()\n",
        "        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
        "        x = jnp.reshape(x, (-1, seqlen, d_head))\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    return compute_attention_heads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TV9ipOTj1eO"
      },
      "source": [
        "def dot_product_self_attention(q, k, v):\n",
        "    \"\"\" Masked dot product self attention.\n",
        "    Args:\n",
        "        q (jax.interpreters.xla.DeviceArray): queries.\n",
        "        k (jax.interpreters.xla.DeviceArray): keys.\n",
        "        v (jax.interpreters.xla.DeviceArray): values.\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
        "    mask_size = q.shape[-2]\n",
        "\n",
        "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
        "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
        "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
        "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
        "    \n",
        "    return DotProductAttention(q, k, v, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRnuY387j1g5"
      },
      "source": [
        "def compute_attention_output_closure(n_heads, d_head):\n",
        "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
        "    Args:\n",
        "        d_head (int):  dimensionality of heads.\n",
        "        n_heads (int): number of attention heads.\n",
        "    Returns:\n",
        "        function: compute_attention_output function\n",
        "    \"\"\"\n",
        "    \n",
        "    def compute_attention_output(x):\n",
        "        \"\"\" Compute the attention output.\n",
        "        Args:\n",
        "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "        Returns:\n",
        "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "        \"\"\"\n",
        "\n",
        "        # Length of the sequence\n",
        "        # Should be size of x's first dimension without counting the batch dim\n",
        "        seqlen = x.shape[1]\n",
        "        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
        "        x = jnp.reshape(x, (-1, n_heads, seqlen, d_head))\n",
        "        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
        "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "             \n",
        "        # Reshape to allow to concatenate the heads\n",
        "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
        "    \n",
        "    return compute_attention_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7PUQg5Ij1i7"
      },
      "source": [
        "def CausalAttention(d_feature, \n",
        "                    n_heads, \n",
        "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
        "                    dot_product_self_attention=dot_product_self_attention,\n",
        "                    compute_attention_output_closure=compute_attention_output_closure,\n",
        "                    mode='train'):\n",
        "    \"\"\"Transformer-style multi-headed causal attention.\n",
        "\n",
        "    Args:\n",
        "        d_feature (int):  dimensionality of feature embedding.\n",
        "        n_heads (int): number of attention heads.\n",
        "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
        "        dot_product_self_attention (function): dot_product_self_attention function. \n",
        "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
        "        mode (str): 'train' or 'eval'.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
        "    \"\"\"\n",
        "    \n",
        "    assert d_feature % n_heads == 0\n",
        "    d_head = d_feature // n_heads\n",
        "   \n",
        "    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
        "    # Since you are dealing with closures you might need to call the outer \n",
        "    # function with the correct parameters to get the actual uncalled function.\n",
        "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
        "        \n",
        "\n",
        "    return tl.Serial(\n",
        "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
        "        ),\n",
        "        \n",
        "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
        "        # HINT: The second argument to tl.Fn() is an uncalled function\n",
        "        # Since you are dealing with closures you might need to call the outer \n",
        "        # function with the correct parameters to get the actual uncalled function.\n",
        "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
        "        tl.Dense(d_feature) # Final dense layer\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ieNnvNhj1kb",
        "outputId": "0f85557e-9360-444b-c717-f45bb3cd5b3f"
      },
      "source": [
        "# Take a look at the causal attention model\n",
        "print(CausalAttention(d_feature=512, n_heads=8))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Branch_out3[\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "  ]\n",
            "  DotProductAttn_in3\n",
            "  AttnOutput\n",
            "  Dense_512\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzrlYa9Cj1no"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\n",
        "                 dropout, mode, ff_activation):\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
        "\n",
        "    The input is an activation tensor.\n",
        "\n",
        "    Args:\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        mode (str): 'train' or 'eval'.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create masked multi-head attention block using CausalAttention function\n",
        "    causal_attention = CausalAttention( \n",
        "                        d_model,\n",
        "                        n_heads=n_heads,\n",
        "                        mode=mode\n",
        "                        )\n",
        "\n",
        "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
        "    feed_forward = [ \n",
        "        # Normalize layer inputs\n",
        "        tl.LayerNorm(),\n",
        "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
        "        tl.Dense(d_ff),\n",
        "        # Add activation function passed in as a parameter (you need to call it!)\n",
        "        ff_activation(), # Generally ReLU\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
        "        tl.Dense(d_model),\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "        tl.Dropout(rate=dropout, mode=mode)\n",
        "    ]\n",
        "\n",
        "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
        "    return [\n",
        "      tl.Residual(\n",
        "          # Normalize layer input\n",
        "          tl.LayerNorm(),\n",
        "          # Add causal attention block previously defined (without parentheses)\n",
        "          causal_attention,\n",
        "          # Add dropout with rate and mode specified\n",
        "          tl.Dropout(rate=dropout, mode=mode)\n",
        "        ),\n",
        "      tl.Residual(\n",
        "          # Add feed forward block (without parentheses)\n",
        "          feed_forward\n",
        "        ),\n",
        "      ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsCrn3l9kTvO",
        "outputId": "5b390d37-3cd4-46ba-9b92-a903f6d73b4a"
      },
      "source": [
        "# Take a look at the decoder block\n",
        "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Serial[\n",
            "        Branch_out3[\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "        ]\n",
            "        DotProductAttn_in3\n",
            "        AttnOutput\n",
            "        Dense_512\n",
            "      ]\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "], Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Dense_2048\n",
            "      Serial[\n",
            "        Relu\n",
            "      ]\n",
            "      Dropout\n",
            "      Dense_512\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akgMBlE_kTx3"
      },
      "source": [
        "def TransformerLM(vocab_size=33300,\n",
        "                  d_model=512,\n",
        "                  d_ff=2048,\n",
        "                  n_layers=6,\n",
        "                  n_heads=8,\n",
        "                  dropout=0.1,\n",
        "                  max_len=4096,\n",
        "                  mode='train',\n",
        "                  ff_activation=tl.Relu):\n",
        "    \"\"\"Returns a Transformer language model.\n",
        "\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\n",
        "    decoder part of the overall Transformer.)\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): vocab size.\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_layers (int): number of decoder layers.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        max_len (int): maximum symbol length for positional encoding.\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
        "        to activations over a vocab set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Embedding inputs and positional encoder\n",
        "    positional_encoder = [ \n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\n",
        "        tl.Embedding(vocab_size, d_model),\n",
        "        # Use dropout with rate and mode specified\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        # Add positional encoding layer with maximum input length and mode specified\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
        "\n",
        "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
        "    decoder_blocks = [ \n",
        "        DecoderBlock(d_model, d_ff, n_heads,\n",
        "                    dropout, mode, ff_activation) for _ in range(n_layers)]\n",
        "\n",
        "    # Create the complete model as written in the figure\n",
        "    return tl.Serial(\n",
        "        # Use teacher forcing (feed output of previous step to current step)\n",
        "        tl.ShiftRight(mode=mode), # Specify the mode!\n",
        "        # Add positional encoder\n",
        "        positional_encoder,\n",
        "        # Add decoder blocks\n",
        "        decoder_blocks,\n",
        "        # Normalize layer\n",
        "        tl.LayerNorm(),\n",
        "\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
        "        tl.Dense(vocab_size),\n",
        "        # Get probabilities with Logsoftmax\n",
        "        tl.LogSoftmax(),\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k09clNYEkT1Y",
        "outputId": "011ba390-26e2-46e3-e5e3-3adfd4583f34"
      },
      "source": [
        "# Take a look at the Transformer\n",
        "print(TransformerLM(n_layers=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_33300_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Branch_out3[\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "          ]\n",
            "          DotProductAttn_in3\n",
            "          AttnOutput\n",
            "          Dense_512\n",
            "        ]\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue4nQjKBkT4d"
      },
      "source": [
        "##PART 3: TRAINING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMLuesgDkT5m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d40e4f-a153-4c57-b1a8-c9b20c74a313"
      },
      "source": [
        "##training the model \n",
        "!pip install trax\n",
        "from trax.supervised import training\n",
        "\n",
        "\n",
        "def training_loop(TransformerLM, train_gen, eval_gen, output_dir):\n",
        "    '''\n",
        "    Input:\n",
        "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
        "        train_gen (generator): Training stream of data.\n",
        "        eval_gen (generator): Evaluation stream of data.\n",
        "        output_dir (str): folder to save your file.\n",
        "        \n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop.\n",
        "    '''\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
        "    \n",
        "    train_task = training.TrainTask( \n",
        "      labeled_data=train_gen, # The training generator\n",
        "      loss_layer=tl.WeightedCategoryCrossEntropy(), # Loss function \n",
        "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
        "      lr_schedule=lr_schedule,\n",
        "      n_steps_per_checkpoint=100\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask( \n",
        "      labeled_data=eval_gen, # The evaluation generator\n",
        "      metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()] # CrossEntropyLoss and Accuracy\n",
        "    )\n",
        "\n",
        "    loop = training.Loop(TransformerLM(d_model=512,\n",
        "                                       d_ff=2048,\n",
        "                                       n_layers=6,\n",
        "                                       n_heads=8,\n",
        "                                       mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    \n",
        "    return loop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trax in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (0.12.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.71+cuda111)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.5.0)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from trax) (2.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.25)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->trax) (0.16.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.10.0.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.6)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.62.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (21.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.17.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets->trax) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.53.0)\n",
            "Requirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.42.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.13.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.22.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (12.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "wyjIn_0XkT97",
        "outputId": "af8ddfea-aae4-4274-a3fd-9b8ca6d66d9b"
      },
      "source": [
        "!rm -f /content/drive/MyDrive/Colab Notebooks/Summarization/model.pkl.gz\n",
        "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream, output_dir=\"/content/drive/MyDrive/Colab Notebooks/Summarization\")\n",
        "loop.run(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-ab274a95aa63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -f /content/drive/MyDrive/Colab Notebooks/Summarization/model.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/Summarization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-965d799bbfb6>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(TransformerLM, train_gen, eval_gen, output_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m     eval_task = training.EvalTask( \n\u001b[1;32m     29\u001b[0m       \u001b[0mlabeled_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The evaluation generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeightedCategoryCrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeightedCategoryAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# CrossEntropyLoss and Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/supervised/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, labeled_data, metrics, metric_names, n_eval_batches, sample_batch, export_prefix)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_eval_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_eval_batches\u001b[0m  \u001b[0;31m# pylint: disable=invalid-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_init_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: \n  In call to configurable 'EvalTask' (<class 'trax.supervised.training.EvalTask'>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZGELTXQkT_M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}